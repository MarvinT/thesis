\subsection*{\ac{DBN} latent space morphs}
Using a large corpus of recorded starling song, a compressive \ac{DBN} \cite{hinton2006reducing} is trained to autoencode 400 mS sections of starling song. The Analysis \& Resynthesis Sound Spectrograph (ARSS) package was used to generate the spectrograms and perform the spectrogram inversion. We used >>>PARAMETERS<<<. We created a dataset of XXXXXXX samples of (possibly overlapping) 400 mS long spectrogram slices of size XXxXX = 7040 dimensions. We found that contrast normalizing the spectrogram using a logistic XXXXX on the outputs of the ARSS generated spectrograms. We tested several several layer size configurations and settled on a network with 5 encoding fully-connected hidden layers of sizes 1024, 512, 256, 128, and 64, which is then decoded back into 128, 256, 512, 1024, and finally back into the 7040 dimensional spectrogram space. The network is trained in a greedy layerwise fashion where the first layer is trained, then fixed, then the next layer is trained with the previous layers fixed, and so on until all layers are trained. Once the network has been trained, we can project a 400 mS motif, $A$, into the latent space, which we'll refer to as $DBN(A)$, and then reconstruct the original spectrogram with the decoder network, $DBN^{-1}(DBN(A))$. In fact, the contrastive divergence learning algorithm used by the network tries to minimize the reconstruction error, $A - DBN^{-1}(DBN(A))$. We can also linearly interpolate between latent space representations of two motifs, $A$ and $E$, to generate a smoothly varying path in spectrogram space that morphs between $A$ and $E$. $\text{morph}(A, E, \tau) = DBN^{-1}(\tau DBN(A) + (1 - \tau) DBN(E)) | \tau \in [0, 1]$. For the experimental morphs I used $\tau \in \{ i/127\ |\ i \in [0, 127] \cap \mathbb{N}\}$

\comment{
\subsection*{DBN morphs}
\acp{DNN} are a general class of algorithms that has been developed by the field of machine learning. They attempt to model high-level abstractions in data using layers of connected model neurons in a loosely biologically inspired hierarchical fashion. \acp{DNN} have been shown to have the theoretical ability to fit any non-linear function provided a large enough model is used and currently represent the state of the art for a wide variety of machine learning tasks.
In fact, some of the characteristics of the composite receptive fields of NCM neurons can be reproduced by an unsupervised \ac{DNN} trained to represent starling songs\cite{kozlov2016central}. 


The activation of a higher layer of a \ac{DNN} is able to fully describe an input in a much more compact representation than the input representation. 
Furthermore, interpolating between points in the top layer of the neural network produces continuously varying meaningful results when projected back into the input space\cite{dosovitskiy2014learning} and I use this to interpolate between starling motifs and generate acoustic morphed motifs.
I have trained a \ac{DBN}, a specific \ac{DNN} model on a library of recorded starling songs. I use a contrast normalized spectrogram as the input representation for the \ac{DBN}. I configure the \ac{DBN} to be compressive, reducing the size of the representation at each layer up until the 5th layer with a dimensionality of 64 from the 7040 input dimensions. This gives a concise representation of starling songs and any point in the 64 dimensional space of the top layer of the \ac{DBN} will represent a motif that could be from a starling song. This allows me to generate morphs between 2 example template motifs (A and E) using a simple linear interpolation in this compressed space. $\text{morph}_{AE}(\tau) = DBN^{-1}\big(\tau DBN(A) + (1-\tau) DBN(E)\big)$ where $DBN(A)$ is top layer representation of the \ac{DBN} when presented with motif A and $DBN^{-1}(x)$ is the projection back through the network to recover the network's estimation of the motif that would cause the activation ($x$). For the experimental morphs I used $\tau \in \{ i/127\ |\ i \in [0, 127] \cap \mathbb{N}\}$
There are potential improvements to the network, however, one of the benefits of the binary probabilistic training is that it encourages the 64 dimensions to be independent and linear.
}


\subsection*{Subjects}
European starlings that have been wild-caught in southern California will serve as the experimental subjects. All subjects have full adult plumage when acquired, indicating they were at least one year old. Otherwise, I do not control for age or sex of the subjects. Subjects are housed in a large mixed sex, conspecific aviary with \textit{ad libitum} access to food and water from the time of capture until being moved into the testing chamber. The photoperiod in the aviary and testing chambers followed the seasonal variation in local sunrise and sunset times.

\subsection*{Operant panels}
Our testing boxes are acoustically isolated chambers containing a panel, shown in \ref{fig:chronic}b, with three response ports. I use PyOperant, an open sourced python package we have developed in the Gentner Lab, to reward the birds by raising a hopper of food to reinforce correct responses. Acoustic stimuli is delivered through a small full-range audio speaker mounted behind the panel and out of the subject's view. Water is unrestricted.

\subsection*{Shaping}
I have coded an approximately unsupervised automated procedure as part of Pyoperant for teaching the birds how to interact with the panel and respond to stimuli to receive the food rewards. This autoshaping routine\cite{brown1968auto} consists of multiple stages that helps the bird become accustomed to receiving a food reward from the hopper and interacting with the panel, and automatically transitions the bird into the task. It takes the subjects an average of 3-5 days to complete the whole autoshaping procedure.

\subsection*{Behavior}
\comment{more details, look at one of jordan's papers}
Subjects learn to classify two sets of song motifs (4motifs/set) using a well-developed \ac{2AC} procedure\cite{gentner2003neuronal}. Briefly, subjects initiate each trial by pecking at the center response port to trigger the presentation of the stimuli (chosen at random on each trial). Immediately after playback, the subject must respond to the left or right port to obtain food. For half the motifs, the subject must to peck left, for the other half it must peck right. Correct responses are reinforced with a brief access to food. Incorrect responses are punished by turning off the houselight for a few seconds and no access to food. High response rates can be achieved and stimulus independent response biases can be ameliorated by manipulating the reinforcement schedules or introducing remedial trials according to established procedures. Subjects can initiate trials from sunrise to sunset and food intake and weight is monitored throughout training insure well-being.
Variable ratio of 4, need to get 1-7 in a row correct in order to get rewarded

\subsection*{Double staircase}
Once the subject meets our performance and response rate criteria, I will begin the ratcheting double staircase procedure to estimate the perceptual boundary between pairs of motifs. The procedure works by estimating a window encompassing the boundary and iteratively reducing the range from both sides (independently) based on the subject's performance. The staircase procedure begins by randomly choosing one of the 16 possible motifs pairs, and then selects a motif morph that is outside the window (90\%) or just inside the window (10\%). For an easy trial, the stimuli will be one of the motifs mixed only slightly with the other motif. For the probe trial, the stimuli will be a morph just within the window the procedure believes the perceptual boundary to be in. If the subject gets a probe trial correct, the window edge will advance to the location of the probe trial and further probe trials along this axis will be more difficult. The subjects are rewarded by a variable reinforcement ratio (they must respond correctly up to a variable threshold before being rewarded) so that the birds are forced to perform on each trial but are not necessarily rewarded. This also allows for more trials per day.

\subsection*{Psychometric curve fitting}
I model the behavior as a bernoulli process with the probability of responding left or right determined by a four parameter generalized logistic fit dependent on the morph position. I fit the parameters of the logistic using maximum log likelihood.

\subsection*{psychometric fitting}

maximum likelihood 4 parameter logistic

$P(r) = A + \frac{K - A}{1 + e^{-B(x-M)}}$


To statistically measure how well psychometric parameters are conserved within morph dimension or within a bird, we consider the pairwise distances between all the fit points of subjective equality (M) across all dimensions and subjects. We then measure the \ac{KS} distance between this null distribution and the subset of pairwise distances that only between the M fit of psychometric curves of the same dimension, and separately the subset of pairwise distances of the same bird. Since our observations are pairwise distances and therefore not independent, we bootstrap a null distribution by shuffling $2^{17}$ times. The bootstrapped p-values do not differ qualitatively from the KS test derived p-values. We perform this same analysis on each of the 4 parameters of the psychometric fits.

\subsection*{hold one out networkometric fits}

We examine the amount of information about the relationship between the behaviorally determined psychometric parameters that is stored in the representation of the \ac{DBN} by fitting a logistic regression and extrapolating to a held out dimension. We use the activation of the first 5 hidden layers of the network (the encoding side) as the artificial network representation. We use logistic regression to predict the probability of left vs right using the behaviorally determined psychometric curves scaled such that $A=0$ and $K=1$. We use 16 fold cross validation structured such that all exemplars of a given dimension are held out at once. This provides us with an estimate of how much the other psychometric curves provide information about the shape of the held out dimension. The sum of the mean squared error between the logistic regression's predicted probabilities and the scaled psychometric curves for each held out set provides a distribution of 16 errors for the representation and set of psychometric curves.

We compare this distribution to null distribution of errors obtained by shuffling the morph dimension labels on the psychometric curves using the \ac{KS} distances. We shuffle each starling's psychometric curves $N=??$ times to get a cumulative null distribution of errors. Then we measure the one sided KS distance from the cumulative distribution to each shuffle to estimate the distribution of KS values, which subsequently allows us to estimate the p-values of each fit.

\subsection*{Electrophysiology recording}

We record from \ac{CM}, a secondary auditory region, in lightly anesthetized (7-8mL of 20\% by volume urethane per kg) starlings. We target \ac{CM} using established stereotaxic coordinates of \SI{2500}{um} rostrally, \SI{500}{um} laterally, and 2000-\SI{2500}{um} deep at an angle of $45^\circ$ from the plane between a beak bite bar and ear pins. \comment{Do I have a citation for these coordinates?}

We record extracellularly using a 32 channel silicon neuronexus probe (most recordings done using an electrode with the edge configuration, although some early recordings may have used other configurations). The recordings used the data acquisition hardware and Spike2 software from Cambridge Electronic Design (CED).

We performed post-hoc spike-sorting using mountainsort\cite{mountainsort} which provided a set spike clusters from putative neurons. We convolved the spike trains with a Gaussian $(\sigma=\SI{10}{ms})$ and sampled the function at 50 points during the stimuli presentation $(400 ms)$ to provide a 50 dimensional representation of the neuron's response to the stimuli that preserved spike timing information. We then exclude neurons that did not contain stimulus specific information by performing pairwise logistic regression on the subset of stimuli corresponding to the training motifs or 8 template endpoints of our morph dimensions, validating performance on held out data. We exclude units with average performance lower than 0.6.

We then presented a range of morph and training stimuli, selected so that half of the presentations were equally spaced along each of the interpolated morph dimensions in the latent space and half were equally spaced in perceptual space to ensure adequate sampling near the relevant behavioral boundaries. Since the perceptual space is behaviorally determined from each bird independently, we randomly sampled the birds to get a fixed stimuli set that could be used for all recordings.

\subsection*{Task relevant LDA-like dimensionality reduction}

We concatenate the 50 dimensional single unit representations into a population representation that varies in size from recording to recording. To compress all representations into the same size lower dimensional space, we perform a linear discriminant analysis-like (LDA-like) dimensionality reduction on the data. Instead of the dimensions described in the LDA algorithm, we project the data onto the dimensions determined by logistic regression performed on the endpoints of the 24 possible comparisons ever behaviorally presented to any cohort of birds. This provided a 24 dimensional representation of the population response that contains the spike timing information of individual units that maximally discriminates the 8 endpoint or template stimuli.

\subsection*{hold one out neurometric fits}

We use the 24 dimensional LDA-like representation of the population response to predict the behaviorally determined psychometric parameters as done with the hold one out networkometric fits. For these, when we calculate the MSE across a dimension, since the data sampling isn't deterministic, but rather gathered from a noisy biological system, the the fits are not quite as good and not evenly sampled. However, since the sampling is consistent across the bootstrapped null distribution, this doesn't impact the results.